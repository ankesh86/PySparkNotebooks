{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP89eGIpUJ2zhOZzj9+UxJg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankesh86/PySparkNotebooks/blob/main/ModelEvaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpZCwM6RTRnD",
        "outputId": "08cc1bb3-edf5-461e-c11c-264b2604156c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.4.0\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.4.0) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317122 sha256=f75d2bb4fb844140bb65466ca667669307c506809efeb1f897e8440849661f29\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load file"
      ],
      "metadata": {
        "id": "YvLfmRrDUdWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"sample_data/bank-full.csv\"\n",
        "target_variable_name = \"y\""
      ],
      "metadata": {
        "id": "qxEV6Jg0Tb8F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Session"
      ],
      "metadata": {
        "id": "gnPygaU6UnGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "df = spark.read.csv(filename, header=True, inferSchema=True, sep=';')\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl56y8H2Uh7L",
        "outputId": "fe888b26-ae7c-4d9a-98aa-e438c5848d04"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "|age|         job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
            "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "| 58|  management| married| tertiary|     no|   2143|    yes|  no|unknown|  5|  may|     261|       1|   -1|       0| unknown| no|\n",
            "| 44|  technician|  single|secondary|     no|     29|    yes|  no|unknown|  5|  may|     151|       1|   -1|       0| unknown| no|\n",
            "| 33|entrepreneur| married|secondary|     no|      2|    yes| yes|unknown|  5|  may|      76|       1|   -1|       0| unknown| no|\n",
            "| 47| blue-collar| married|  unknown|     no|   1506|    yes|  no|unknown|  5|  may|      92|       1|   -1|       0| unknown| no|\n",
            "| 33|     unknown|  single|  unknown|     no|      1|     no|  no|unknown|  5|  may|     198|       1|   -1|       0| unknown| no|\n",
            "| 35|  management| married| tertiary|     no|    231|    yes|  no|unknown|  5|  may|     139|       1|   -1|       0| unknown| no|\n",
            "| 28|  management|  single| tertiary|     no|    447|    yes| yes|unknown|  5|  may|     217|       1|   -1|       0| unknown| no|\n",
            "| 42|entrepreneur|divorced| tertiary|    yes|      2|    yes|  no|unknown|  5|  may|     380|       1|   -1|       0| unknown| no|\n",
            "| 58|     retired| married|  primary|     no|    121|    yes|  no|unknown|  5|  may|      50|       1|   -1|       0| unknown| no|\n",
            "| 43|  technician|  single|secondary|     no|    593|    yes|  no|unknown|  5|  may|      55|       1|   -1|       0| unknown| no|\n",
            "| 41|      admin.|divorced|secondary|     no|    270|    yes|  no|unknown|  5|  may|     222|       1|   -1|       0| unknown| no|\n",
            "| 29|      admin.|  single|secondary|     no|    390|    yes|  no|unknown|  5|  may|     137|       1|   -1|       0| unknown| no|\n",
            "| 53|  technician| married|secondary|     no|      6|    yes|  no|unknown|  5|  may|     517|       1|   -1|       0| unknown| no|\n",
            "| 58|  technician| married|  unknown|     no|     71|    yes|  no|unknown|  5|  may|      71|       1|   -1|       0| unknown| no|\n",
            "| 57|    services| married|secondary|     no|    162|    yes|  no|unknown|  5|  may|     174|       1|   -1|       0| unknown| no|\n",
            "| 51|     retired| married|  primary|     no|    229|    yes|  no|unknown|  5|  may|     353|       1|   -1|       0| unknown| no|\n",
            "| 45|      admin.|  single|  unknown|     no|     13|    yes|  no|unknown|  5|  may|      98|       1|   -1|       0| unknown| no|\n",
            "| 57| blue-collar| married|  primary|     no|     52|    yes|  no|unknown|  5|  may|      38|       1|   -1|       0| unknown| no|\n",
            "| 60|     retired| married|  primary|     no|     60|    yes|  no|unknown|  5|  may|     219|       1|   -1|       0| unknown| no|\n",
            "| 33|    services| married|secondary|     no|      0|    yes|  no|unknown|  5|  may|      54|       1|   -1|       0| unknown| no|\n",
            "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df = df.withColumn('label', F.when(F.col(\"y\") == 'yes', 1).otherwise(0))\n",
        "df = df.drop('y')"
      ],
      "metadata": {
        "id": "lRrUQJgfUmaW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Random sampling"
      ],
      "metadata": {
        "id": "GMPB1xFJUzep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = df.randomSplit([0.7, 0.3], seed = 12345)"
      ],
      "metadata": {
        "id": "UFipdU_PUwq2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in df.dtypes:\n",
        "    if v not in ['string']:\n",
        "        print(k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9455YigVSs2",
        "outputId": "acecdbac-15b2-447d-fbed-5acf18455763"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age\n",
            "balance\n",
            "day\n",
            "duration\n",
            "campaign\n",
            "pdays\n",
            "previous\n",
            "label\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.select(['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous', 'label'])"
      ],
      "metadata": {
        "id": "FHNTAia7VU6F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "#assemble individual columns to one column - 'features'\n",
        "def assemble_vectors(df, features_list, target_variable_name):\n",
        "    stages = []\n",
        "    #assemble vectors\n",
        "    assembler = VectorAssembler(inputCols=features_list, outputCol='features')\n",
        "    stages = [assembler]\n",
        "    #select all the columns + target + newly created 'features' column\n",
        "    selectedCols = [target_variable_name, 'features']\n",
        "    #use pipeline to process sequentially\n",
        "    pipeline = Pipeline(stages=stages)\n",
        "    #assembler model\n",
        "    assembleModel = pipeline.fit(df)\n",
        "    #apply assembler model on data\n",
        "    df = assembleModel.transform(df).select(selectedCols)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "RgxtOLE7VX4y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#exclude target variable and select all other feature vectors\n",
        "features_list = df.columns\n",
        "#features_list = char_vars #this option is used only for ChiSqselector\n",
        "features_list.remove('label')\n",
        "# apply the function on our dataframe\n",
        "assembled_df = assemble_vectors(train, features_list, 'label')"
      ],
      "metadata": {
        "id": "fFicDYbvVdY2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using ml tuning library"
      ],
      "metadata": {
        "id": "g-ONTxPjV9gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "\n",
        "#model initialization\n",
        "lr = LogisticRegression(maxIter=10, featuresCol='features', labelCol='label')\n",
        "\n",
        "#model parameters to try\n",
        "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1,0.01]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).build()\n",
        "\n",
        "# 70% of data will be used for taining, 30% for validation\n",
        "train_valid_clf = TrainValidationSplit(estimator=lr, estimatorParamMaps=paramGrid, evaluator=BinaryClassificationEvaluator(), trainRatio=0.7)\n",
        "\n",
        "#assembled_df is the output of the vector assembler\n",
        "model = train_valid_clf.fit(assembled_df)"
      ],
      "metadata": {
        "id": "kZflrgrOV537"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Straightified Sampling"
      ],
      "metadata": {
        "id": "VvC4kMToZeNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split data for 0s and 1s\n",
        "zero_df = df.filter(df[\"label\"]==0)\n",
        "one_df = df.filter(df[\"label\"]==1)\n",
        "\n",
        "# split data into train and test\n",
        "train_zero, test_zero = zero_df.randomSplit([0.7,0.3], seed=12345)\n",
        "train_one, test_one = one_df.randomSplit([0.7,0.3], seed=12345)\n",
        "\n",
        "# union datasets\n",
        "train = train_zero.union(train_one)\n",
        "test = test_zero.union(test_one)"
      ],
      "metadata": {
        "id": "roTz7BDoXb7n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-Fold cross validation\n",
        "using ML tuning library"
      ],
      "metadata": {
        "id": "gxsYOYUlZkId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "#model initialization\n",
        "lr = LogisticRegression(maxIter=10, featuresCol='features', labelCol='label')\n",
        "\n",
        "#model parameters to try\n",
        "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01]).addGrid(lr.elasticNetParam, [0.0,0.5,1.0]).build()\n",
        "\n",
        "#number of folds = 3\n",
        "crossval_clf = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=BinaryClassificationEvaluator(), numFolds=3)\n",
        "\n",
        "#assembled_df is the output of the vector assembler\n",
        "model = crossval_clf.fit(assembled_df)"
      ],
      "metadata": {
        "id": "37hclnvMZicf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Holdout**"
      ],
      "metadata": {
        "id": "WAw1unbXcavp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test, holdout = df.randomSplit([0.7, 0.2, 0.1], seed=12345)\n",
        "train.count(), test.count(), holdout.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwpLe2e8a0HE",
        "outputId": "a95e3f5a-c5d2-43d9-8cae-b47c30c7f646"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31527, 9051, 4633)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Leave-one Group-out**"
      ],
      "metadata": {
        "id": "LeN-1O3Hc3_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import countDistinct\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "filename = 'sample_data/bank-full.csv'\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "df = spark.read.csv(filename, header=True, inferSchema=True, sep=';')\n",
        "df = df.withColumn('label', F.when(F.col(\"y\") == 'yes', 1).otherwise(0))\n",
        "df.drop('y')\n",
        "\n",
        "df = df.select(['education','age','balance','day','duration','campaign','pdays','previous','label'])\n",
        "features_list = ['age','balance','day','duration','campaign','pdays','previous']\n",
        "\n",
        "#assemble individual column to one columns\n",
        "def assemble_vectors(df,features_list, target_variable_name, group_variable_name):\n",
        "  stages = []\n",
        "  #assemble vectors\n",
        "  assembler = VectorAssembler(inputCols=features_list, outputCol='features')\n",
        "  stages=[assembler]\n",
        "\n",
        "  selectedCols = [group_variable_name, target_variable_name, 'features']\n",
        "\n",
        "  #use pipeline to process sequentially\n",
        "  pipeline = Pipeline(stages=stages)\n",
        "\n",
        "  #assemble Model\n",
        "  assembleModel = pipeline.fit(df)\n",
        "\n",
        "  # apply assemble model on data\n",
        "  df = assembleModel.transform(df).select(selectedCols)\n",
        "\n",
        "  return df\n",
        "\n",
        "# apply the functon on our dataframe\n",
        "joined_df = assemble_vectors(df, features_list, 'label', 'education')\n",
        "\n",
        "#find the groups to apply cross validation\n",
        "groups = list(joined_df.select('education').toPandas()['education'].unique())\n",
        "\n",
        "# leave-one-group-out validation\n",
        "def leave_one_group_out_validator(df, var_name, groups):\n",
        "  train_metric_score =  []\n",
        "  test_metric_score =  []\n",
        "\n",
        "  for i in groups:\n",
        "    train = df.filter(df[var_name] != i)\n",
        "    test = df.filter(df[var_name]==i)\n",
        "\n",
        "    #model initialization\n",
        "    lr = LogisticRegression(maxIter = 10, featuresCol='features', labelCol='label')\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "\n",
        "    #fit model\n",
        "    lrModel = lr.fit(train)\n",
        "\n",
        "    #make predictions\n",
        "    predict_train = lrModel.transform(train)\n",
        "    predict_test = lrModel.transform(test)\n",
        "    train_metric_score.append(evaluator.evaluate(predict_train))\n",
        "    test_metric_score.append(evaluator.evaluate(predict_test))\n",
        "    print(str(i) + \"Group Evaluation\")\n",
        "    print(\" Train AUC - \", train_metric_score[-1])\n",
        "    print(\" Test AUC - \", test_metric_score[-1])\n",
        "\n",
        "  print('Final evaluation for model')\n",
        "  print('Train ROC', np.mean(train_metric_score))\n",
        "  print('Test ROC', np.mean(test_metric_score))"
      ],
      "metadata": {
        "id": "du6yIKGdc0w_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "leave_one_group_out_validator(joined_df, 'education', groups)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s16Kq0LqivAR",
        "outputId": "8a40fe72-6e44-409a-efdd-31b07bd85568"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tertiaryGroup Evaluation\n",
            " Train AUC -  0.8405036511556044\n",
            " Test AUC -  0.8117014214186873\n",
            "secondaryGroup Evaluation\n",
            " Train AUC -  0.8249617583391624\n",
            " Test AUC -  0.8352564395071835\n",
            "unknownGroup Evaluation\n",
            " Train AUC -  0.8314701472433579\n",
            " Test AUC -  0.8111358354349009\n",
            "primaryGroup Evaluation\n",
            " Train AUC -  0.8256814880628227\n",
            " Test AUC -  0.8695884216387456\n",
            "Final evaluation for model\n",
            "Train ROC 0.8306542612002368\n",
            "Test ROC 0.8319205294998793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qn84xWqpffLo"
      }
    }
  ]
}