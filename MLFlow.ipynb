{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd598f7-bb3b-4dc2-a7bc-d4dd19fb6124",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\nbalance\nday\nduration\ncampaign\npdays\nprevious\nlabel\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import sys\n",
    "import time\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Start a Spark session specifically for this job\n",
    "spark = SparkSession.builder.appName(\"mlflow_example\").getOrCreate()\n",
    "\n",
    "# Path to your dataset\n",
    "filename = \"/FileStore/tables/bank_full.csv\"\n",
    "\n",
    "target_variable_name = \"y\"\n",
    "from pyspark.sql import functions as F\n",
    "df = spark.read.csv(filename, header=True, inferSchema=True, sep=';')\n",
    "df = df.withColumn('label', F.when(F.col(\"y\") == 'yes', 1).otherwise(0))\n",
    "df = df.drop('y')\n",
    "train, test = df.randomSplit([0.7, 0.3], seed=12345)\n",
    "\n",
    "for k, v in df.dtypes:\n",
    "    if v not in ['string']:\n",
    "        print(k)\n",
    "\n",
    "df = df.select(['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous', 'label'])\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#assemble individual columns to one column - 'features'\n",
    "def assemble_vectors(df, features_list, target_variable_name):\n",
    "    stages = []\n",
    "    #assemble vectors\n",
    "    assembler = VectorAssembler(inputCols=features_list, outputCol='features')\n",
    "    stages = [assembler]\n",
    "    #select all the columns + target + newly created 'features' column\n",
    "    selectedCols = [target_variable_name, 'features']\n",
    "    #use pipeline to process sequentially\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    #assembler model\n",
    "    assembleModel = pipeline.fit(df)\n",
    "    #apply assembler model on data\n",
    "    df = assembleModel.transform(df).select(selectedCols)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90de1b42-224e-41b2-82e7-c3a87dbb71d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#exclude target variable and select all other feature vectors\n",
    "features_list = df.columns\n",
    "#features_list = char_vars #this option is used only for ChiSqselector\n",
    "features_list.remove('label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0cd61f-ce7d-4f93-828b-6b6bd456090e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31126dff1754152ac6c69cf70f44207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0117992c632949e2ab0e78b9e0b0a92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# apply the function on our dataframe\n",
    "assembled_train_df = assemble_vectors(train, features_list, 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e13edb-ab9e-435f-b3bb-2a8465a68f60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc442cc94ca34e40b25af9e9ea02566f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccad845f24342de90dd22952f900713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assembled_test_df = assemble_vectors(test, features_list, 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e880c50-fa36-4658-9226-51dc71826a40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b205c03-c720-447d-830a-cc1e0f1ab23a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(sys.argv[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a5e214d-4bf1-44f2-8db4-fd49efbd9d03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(sys.argv[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de2ae449-99ea-452c-8a04-6fde6d8ad4c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assembled_test_df = assemble_vectors(test, features_list, target_variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f00c6a40-1a45-4adf-bf21-74fed80b3a54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/16 01:36:32 WARNING mlflow.spark: With Pyspark >= 3.2, PYSPARK_PIN_THREAD environment variable must be set to false for Spark datasource autologging to work.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RFModel\nCompleted training RFModel\nTest Area Under ROC: 0.8452161588891514\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/16 01:36:55 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376a502c04774031877e3474049d79d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/16 01:36:56 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n2024/05/16 01:37:27 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/2770898139939067/9dacb2f386984eb99371005a025be97e/artifacts/model/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871728a2b24b443787363f4551295ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbfs:/databricks/mlflow-tracking/2770898139939067/9dacb2f386984eb99371005a025be97e/artifacts\n"
     ]
    }
   ],
   "source": [
    "# Set the MLflow tracking URI and set up the experiment\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(\"/Shared/MLflow Example\")\n",
    "\n",
    "# Automatically log all parameters, metrics, and models to MLflow\n",
    "mlflow.spark.autolog()\n",
    "\n",
    "# Model hyperparameters\n",
    "maxBinsVal = 32 #float(sys.argv[1]) if len(sys.argv) > 3 else 20\n",
    "maxDepthVal = 5 #float(sys.argv[2]) if len(sys.argv) > 3 else 5\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    classifier = RandomForestClassifier(labelCol='label', featuresCol='features', maxBins=maxBinsVal, maxDepth=maxDepthVal)\n",
    "    stages_tree = [classifier]\n",
    "    pipeline_tree = Pipeline(stages=stages_tree)\n",
    "\n",
    "    # Train the model\n",
    "    print('Running RFModel')\n",
    "    RFmodel = pipeline_tree.fit(assembled_train_df)\n",
    "    print('Completed training RFModel')\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = RFmodel.transform(assembled_test_df)\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "    # Evaluate the model\n",
    "    roc_auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    print(\"Test Area Under ROC: \" + str(roc_auc))\n",
    "\n",
    "    # Log model and metrics\n",
    "    mlflow.log_param(\"maxBins\", maxBinsVal)\n",
    "    mlflow.log_param(\"maxDepth\", maxDepthVal)\n",
    "    mlflow.log_metric(\"ROC\", roc_auc)\n",
    "    mlflow.spark.log_model(RFmodel, \"model\")\n",
    "    print(mlflow.get_artifact_uri())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b86baafa-ff04-4a05-b30a-0dd8b796a20a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MLFlow",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
