{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNczA7GYttG1cyMEKFRJcgW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankesh86/PySparkNotebooks/blob/main/MLlib_Packages_chi_sq_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aft9nIL5j5Xt",
        "outputId": "1a8dbbcb-4224-4b5d-ca55-44389b486513"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.4.0\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.4.0) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317122 sha256=8361c4733902d3fa1378577e64744a96d3784e2a01abb77db99862fdbcf9c370\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pl25QrXjh1vS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('basic_stats').getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls sample_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDnNnBXLi6um",
        "outputId": "a6b3b36d-b667-4cf4-e702-0b7b58e74896"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;32manscombe.json\u001b[0m*               california_housing_train.csv  mnist_test.csv         \u001b[01;32mREADME.md\u001b[0m*\n",
            "california_housing_test.csv  chi_sq.csv                    mnist_train_small.csv  Salary_Data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('sample_data/Salary_Data.csv',header = True, inferSchema = True)\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRhdWdXRlkcR",
        "outputId": "d956e4d8-6486-4811-c75d-bfc9b0833134"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_eLiso3qmvt",
        "outputId": "5417de61-1575-44e9-cf17-ed630e675a65"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------+\n",
            "|YearsExperience| Salary|\n",
            "+---------------+-------+\n",
            "|            1.1|39343.0|\n",
            "|            1.3|46205.0|\n",
            "|            1.5|37731.0|\n",
            "|            2.0|43525.0|\n",
            "|            2.2|39891.0|\n",
            "|            2.9|56642.0|\n",
            "|            3.0|60150.0|\n",
            "|            3.2|54445.0|\n",
            "|            3.2|64445.0|\n",
            "|            3.7|57189.0|\n",
            "|            3.9|63218.0|\n",
            "|            4.0|55794.0|\n",
            "|            4.0|56957.0|\n",
            "|            4.1|57081.0|\n",
            "|            4.5|61111.0|\n",
            "|            4.9|67938.0|\n",
            "|            5.1|66029.0|\n",
            "|            5.3|83088.0|\n",
            "|            5.9|81363.0|\n",
            "|            6.0|93940.0|\n",
            "+---------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=df.columns, outputCol=\"features\")\n",
        "df_new = assembler.transform(df)\n",
        "df_new.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0d2yQGUqzs4",
        "outputId": "df45868b-3b00-42d0-f4fb-8bb68470f5ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------+-------------+\n",
            "|YearsExperience| Salary|     features|\n",
            "+---------------+-------+-------------+\n",
            "|            1.1|39343.0|[1.1,39343.0]|\n",
            "|            1.3|46205.0|[1.3,46205.0]|\n",
            "|            1.5|37731.0|[1.5,37731.0]|\n",
            "|            2.0|43525.0|[2.0,43525.0]|\n",
            "|            2.2|39891.0|[2.2,39891.0]|\n",
            "|            2.9|56642.0|[2.9,56642.0]|\n",
            "|            3.0|60150.0|[3.0,60150.0]|\n",
            "|            3.2|54445.0|[3.2,54445.0]|\n",
            "|            3.2|64445.0|[3.2,64445.0]|\n",
            "|            3.7|57189.0|[3.7,57189.0]|\n",
            "|            3.9|63218.0|[3.9,63218.0]|\n",
            "|            4.0|55794.0|[4.0,55794.0]|\n",
            "|            4.0|56957.0|[4.0,56957.0]|\n",
            "|            4.1|57081.0|[4.1,57081.0]|\n",
            "|            4.5|61111.0|[4.5,61111.0]|\n",
            "|            4.9|67938.0|[4.9,67938.0]|\n",
            "|            5.1|66029.0|[5.1,66029.0]|\n",
            "|            5.3|83088.0|[5.3,83088.0]|\n",
            "|            5.9|81363.0|[5.9,81363.0]|\n",
            "|            6.0|93940.0|[6.0,93940.0]|\n",
            "+---------------+-------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pearson Cofficient of Correlation"
      ],
      "metadata": {
        "id": "V9HtUrMysk4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.stat import Correlation\n",
        "pearson_corr = Correlation.corr(df_new,'features')\n",
        "pearson_corr.show(2,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRC27JFxrv7F",
        "outputId": "715a1c0b-11a3-4d9d-8022-0c7034ffef74"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------+\n",
            "|pearson(features)                                                                 |\n",
            "+----------------------------------------------------------------------------------+\n",
            "|1.0                 0.9782416184887599  \\n0.9782416184887599  1.0                 |\n",
            "+----------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spearman Cofficient of Correlation"
      ],
      "metadata": {
        "id": "4d_6KkCOtzhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spearman_corr = Correlation.corr(df_new,'features',\"spearman\")\n",
        "spearman_corr.show(2,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu7C4tw5sgg1",
        "outputId": "2e69ef73-10c4-4ce1-d294-d1e04fc0d32c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------+\n",
            "|spearman(features)                                                                |\n",
            "+----------------------------------------------------------------------------------+\n",
            "|1.0                 0.9568313543516996  \\n0.9568313543516996  1.0                 |\n",
            "+----------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chi-Squate Test"
      ],
      "metadata": {
        "id": "_NEDVQnhwyKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('sample_data/chi_sq.csv',inferSchema=True,header=True)\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0G0zCK1uG4H",
        "outputId": "7cbf46c3-ea36-4273-95dd-4bfc7ce54302"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45211"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SokDbtJIeLv1",
        "outputId": "96d25327-54fa-4384-b3b7-763fe3e29d18"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+-----+\n",
            "| marital|housing|label|\n",
            "+--------+-------+-----+\n",
            "| married|    yes|    0|\n",
            "|  single|    yes|    0|\n",
            "| married|    yes|    0|\n",
            "| married|    yes|    0|\n",
            "|  single|     no|    0|\n",
            "| married|    yes|    0|\n",
            "|  single|    yes|    0|\n",
            "|divorced|    yes|    0|\n",
            "| married|    yes|    0|\n",
            "|  single|    yes|    0|\n",
            "|divorced|    yes|    0|\n",
            "|  single|    yes|    0|\n",
            "| married|    yes|    0|\n",
            "| married|    yes|    0|\n",
            "| married|    yes|    0|\n",
            "| married|    yes|    0|\n",
            "|  single|    yes|    0|\n",
            "| married|    yes|    0|\n",
            "| married|    yes|    0|\n",
            "| married|    yes|    0|\n",
            "+--------+-------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Assuming you already have a SparkSession instantiated and a DataFrame df loaded\n",
        "\n",
        "# Configure an indexer for the marital status\n",
        "marital_indexer = StringIndexer(inputCol=\"marital\", outputCol=\"marital_num\")\n",
        "# Configure the OneHotEncoder to work on the indexed column\n",
        "marital_encoder = OneHotEncoder(inputCols=[\"marital_num\"], outputCols=[\"marital_vector\"])\n",
        "\n",
        "# Configure an indexer for the housing status\n",
        "housing_indexer = StringIndexer(inputCol=\"housing\", outputCol=\"housing_num\")\n",
        "# Configure the OneHotEncoder to work on the indexed column\n",
        "housing_encoder = OneHotEncoder(inputCols=[\"housing_num\"], outputCols=[\"housing_vector\"])\n",
        "\n",
        "# Build the pipeline\n",
        "pipeline = Pipeline(stages=[marital_indexer, marital_encoder, housing_indexer, housing_encoder])\n",
        "\n",
        "# Fit the pipeline to the data\n",
        "model = pipeline.fit(df)\n",
        "\n",
        "# Transform the data using the pipeline\n",
        "df = model.transform(df)\n",
        "\n",
        "# Show the transformed DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqEHT7tzeoL_",
        "outputId": "76f95d3e-e5c5-4883-8507-712595e179a0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+-----+-----------+--------------+-----------+--------------+\n",
            "| marital|housing|label|marital_num|marital_vector|housing_num|housing_vector|\n",
            "+--------+-------+-----+-----------+--------------+-----------+--------------+\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "|  single|    yes|    0|        1.0| (2,[1],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "|  single|     no|    0|        1.0| (2,[1],[1.0])|        1.0|     (1,[],[])|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "|  single|    yes|    0|        1.0| (2,[1],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "|divorced|    yes|    0|        2.0|     (2,[],[])|        0.0| (1,[0],[1.0])|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "|  single|    yes|    0|        1.0| (2,[1],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "|divorced|    yes|    0|        2.0|     (2,[],[])|        0.0| (1,[0],[1.0])|\n",
            "|  single|    yes|    0|        1.0| (2,[1],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "|  single|    yes|    0|        1.0| (2,[1],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|\n",
            "+--------+-------+-----+-----------+--------------+-----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_assembler = VectorAssembler(inputCols=['marital_vector','housing_vector'],outputCol='features')\n",
        "df = df_assembler.transform(df)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCL8KM74gzF-",
        "outputId": "22ddf1f5-fa78-478c-ff0f-e190e5054534"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+-----+-----------+--------------+-----------+--------------+-------------+\n",
            "| marital|housing|label|marital_num|marital_vector|housing_num|housing_vector|     features|\n",
            "+--------+-------+-----+-----------+--------------+-----------+--------------+-------------+\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|[1.0,0.0,1.0]|\n",
            "|  single|    yes|    0|        1.0| (2,[1],[1.0])|        0.0| (1,[0],[1.0])|[0.0,1.0,1.0]|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|[1.0,0.0,1.0]|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|[1.0,0.0,1.0]|\n",
            "|  single|     no|    0|        1.0| (2,[1],[1.0])|        1.0|     (1,[],[])|[0.0,1.0,0.0]|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|[1.0,0.0,1.0]|\n",
            "|  single|    yes|    0|        1.0| (2,[1],[1.0])|        0.0| (1,[0],[1.0])|[0.0,1.0,1.0]|\n",
            "|divorced|    yes|    0|        2.0|     (2,[],[])|        0.0| (1,[0],[1.0])|[0.0,0.0,1.0]|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|[1.0,0.0,1.0]|\n",
            "|  single|    yes|    0|        1.0| (2,[1],[1.0])|        0.0| (1,[0],[1.0])|[0.0,1.0,1.0]|\n",
            "|divorced|    yes|    0|        2.0|     (2,[],[])|        0.0| (1,[0],[1.0])|[0.0,0.0,1.0]|\n",
            "|  single|    yes|    0|        1.0| (2,[1],[1.0])|        0.0| (1,[0],[1.0])|[0.0,1.0,1.0]|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|[1.0,0.0,1.0]|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|[1.0,0.0,1.0]|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|[1.0,0.0,1.0]|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|[1.0,0.0,1.0]|\n",
            "|  single|    yes|    0|        1.0| (2,[1],[1.0])|        0.0| (1,[0],[1.0])|[0.0,1.0,1.0]|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|[1.0,0.0,1.0]|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|[1.0,0.0,1.0]|\n",
            "| married|    yes|    0|        0.0| (2,[0],[1.0])|        0.0| (1,[0],[1.0])|[1.0,0.0,1.0]|\n",
            "+--------+-------+-----+-----------+--------------+-----------+--------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.stat import ChiSquareTest\n",
        "chi_sq = ChiSquareTest.test(df,\"features\",\"label\").head()\n",
        "print(\"pValues: \" + str(chi_sq.pValues))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5Ad0z3fiSbs",
        "outputId": "f98888a8-c279-4b0c-e819-dbd5ac73ee0a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pValues: [1.0,1.0,1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the degrees of freedom\n",
        "print(\"degreesOfFreedom: \" + str(chi_sq.degreesOfFreedom))\n",
        "print(\"statistics: \" + str(chi_sq.statistics))\n",
        "print(\"pValues: \" + str(chi_sq.pValues))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAHqo2HOkF6F",
        "outputId": "15556b56-1132-46cb-e912-3e9e1b2b9f35"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "degreesOfFreedom: [0, 0, 0]\n",
            "statistics: [0.0,0.0,0.0]\n",
            "pValues: [1.0,1.0,1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **The DataFrame you've shown appears to be related to some preprocessing steps commonly used in machine learning to prepare categorical data for modeling. Let me break down the information you have provided:**\n",
        "\n",
        "marital, housing: These columns represent categorical variables in their original string form.\n",
        "\n",
        "label: This is typically the target variable you want to predict, with '0' possibly indicating a negative class and '1' indicating a positive class.\n",
        "\n",
        "marital_num, housing_num: These columns represent the numerical encoding of the categorical variables. This is a necessary step because most machine learning algorithms work with numerical data, not strings. For instance, 'married' might be coded as 0.0, 'single' as 1.0, and 'divorced' as 2.0.\n",
        "\n",
        "marital_vector, housing_vector: These columns represent the one-hot encoded vectors corresponding to the numerical encoding of the categorical variables. One-hot encoding is a process that converts categorical variables into a form that could be provided to ML algorithms to do a better job in prediction. It creates a binary column for each category and returns a sparse matrix or dense array (depending on the input data).\n",
        "\n",
        "In marital_vector and housing_vector, the structure (2,[0],[1.0]) might be explained as follows:\n",
        "\n",
        "2 indicates the size of the vector. It means there are two possible values.\n",
        "\n",
        "[0] indicates the index of the array where the 1.0 is placed, which represents the presence of a particular category.\n",
        "\n",
        "[1.0] indicates the value at that index position. A '1' in one of these positions indicates the presence of that particular category, while all other positions will be '0'.\n",
        "This is the \"one-hot\" part: only one position is \"hot\" (1) for each observation.\n",
        "\n",
        "features: This is typically a combined feature vector that would be used as input for machine learning models. It combines all the individual vectors into a single feature vector.\n",
        "\n",
        "## **Why Use OneHotEncoder?**\n",
        "\n",
        "OneHotEncoder is used because many machine learning algorithms cannot work directly with categorical data. If you encode the categories as integers, the model may misinterpret the data to be in some kind of order, 0 < 1 < 2, which is not the case.\n",
        "\n",
        "The one-hot encoding creates a binary column for each category and returns a matrix with 1s and 0s. It's a way of telling the model \"this data is categorical, not numerical\".\n",
        "\n",
        "Using OneHotEncoding prevents the model from assuming a natural ordering between categories which can result in poor performance or unexpected results (for example, it would be incorrect to think that 'married' < 'single' < 'divorced' just because 0 < 1 < 2).\n",
        "\n",
        "### **In summary, the numerical transformation (marital_num, housing_num) is an intermediate step that is often followed by one-hot encoding (marital_vector, housing_vector) to create binary variables that represent the presence of each possible category. The final features column is a vector that includes all these binary variables as a single input feature for modeling purposes.**"
      ],
      "metadata": {
        "id": "XLWYH_uJo0eC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Binarizer**"
      ],
      "metadata": {
        "id": "mzqO6fgssPTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Binarizer\n",
        "binarizer = Binarizer(threshold=0.99, inputCol=\"label\", outputCol=\"binarized_label\")\n",
        "new_df = binarizer.transform(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "CVijlta1nvMZ",
        "outputId": "79e146d6-2eda-49f5-964c-c356b37b362d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "Data type IntegerType is not supported.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-041599da3385>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbinarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binarized_label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: Data type IntegerType is not supported."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Different pre-processing operations in ML**\n",
        "* **Binarizer** - making values 0 or 1\n",
        "* **Principal Component Analysis** - reduces the dimension\n",
        "* **Normalizer** - mean 0 and SD 1\n",
        "* **Standard Scaling** - normalise data between [0,1]\n",
        "* **min-Max Scaling** - we can specify limits based on max and min in DF\n",
        "* **MaxAbsScaler** - rescales feature values between -1 and 1\n",
        "* **Binning** - bucketing, (Bucketizer) make data in category"
      ],
      "metadata": {
        "id": "ro5Vi0brvEwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-qa4OGjspm4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}